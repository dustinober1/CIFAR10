{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7ee81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b789b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 170498071/170498071 [00:02<00:00, 63931441.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ed9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77060ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 2000, Loss: 2.021, Accuracy: 21.48%\n",
      "Epoch: 1, Batch: 4000, Loss: 1.739, Accuracy: 27.21%\n",
      "Epoch: 1, Batch: 6000, Loss: 1.600, Accuracy: 31.71%\n",
      "Epoch: 1, Batch: 8000, Loss: 1.489, Accuracy: 35.16%\n",
      "Epoch: 1, Batch: 10000, Loss: 1.404, Accuracy: 38.02%\n",
      "Epoch: 1, Batch: 12000, Loss: 1.357, Accuracy: 40.28%\n",
      "Epoch: 2, Batch: 2000, Loss: 1.284, Accuracy: 54.95%\n",
      "Epoch: 2, Batch: 4000, Loss: 1.232, Accuracy: 55.84%\n",
      "Epoch: 2, Batch: 6000, Loss: 1.185, Accuracy: 56.75%\n",
      "Epoch: 2, Batch: 8000, Loss: 1.157, Accuracy: 57.61%\n",
      "Epoch: 2, Batch: 10000, Loss: 1.148, Accuracy: 58.28%\n",
      "Epoch: 2, Batch: 12000, Loss: 1.136, Accuracy: 58.80%\n",
      "Epoch: 3, Batch: 2000, Loss: 1.027, Accuracy: 65.29%\n",
      "Epoch: 3, Batch: 4000, Loss: 1.025, Accuracy: 65.09%\n",
      "Epoch: 3, Batch: 6000, Loss: 1.017, Accuracy: 65.21%\n",
      "Epoch: 3, Batch: 8000, Loss: 1.010, Accuracy: 65.50%\n",
      "Epoch: 3, Batch: 10000, Loss: 0.983, Accuracy: 65.78%\n",
      "Epoch: 3, Batch: 12000, Loss: 0.985, Accuracy: 66.00%\n",
      "Epoch: 4, Batch: 2000, Loss: 0.928, Accuracy: 69.47%\n",
      "Epoch: 4, Batch: 4000, Loss: 0.911, Accuracy: 69.25%\n",
      "Epoch: 4, Batch: 6000, Loss: 0.906, Accuracy: 69.58%\n",
      "Epoch: 4, Batch: 8000, Loss: 0.875, Accuracy: 69.97%\n",
      "Epoch: 4, Batch: 10000, Loss: 0.882, Accuracy: 70.11%\n",
      "Epoch: 4, Batch: 12000, Loss: 0.874, Accuracy: 70.25%\n",
      "Epoch: 5, Batch: 2000, Loss: 0.813, Accuracy: 72.42%\n",
      "Epoch: 5, Batch: 4000, Loss: 0.825, Accuracy: 72.49%\n",
      "Epoch: 5, Batch: 6000, Loss: 0.796, Accuracy: 72.82%\n",
      "Epoch: 5, Batch: 8000, Loss: 0.823, Accuracy: 72.77%\n",
      "Epoch: 5, Batch: 10000, Loss: 0.800, Accuracy: 72.89%\n",
      "Epoch: 5, Batch: 12000, Loss: 0.801, Accuracy: 72.93%\n",
      "Epoch: 6, Batch: 2000, Loss: 0.738, Accuracy: 76.17%\n",
      "Epoch: 6, Batch: 4000, Loss: 0.717, Accuracy: 76.34%\n",
      "Epoch: 6, Batch: 6000, Loss: 0.743, Accuracy: 75.94%\n",
      "Epoch: 6, Batch: 8000, Loss: 0.728, Accuracy: 75.73%\n",
      "Epoch: 6, Batch: 10000, Loss: 0.726, Accuracy: 75.83%\n",
      "Epoch: 6, Batch: 12000, Loss: 0.721, Accuracy: 75.83%\n",
      "Epoch: 7, Batch: 2000, Loss: 0.690, Accuracy: 77.03%\n",
      "Epoch: 7, Batch: 4000, Loss: 0.671, Accuracy: 77.64%\n",
      "Epoch: 7, Batch: 6000, Loss: 0.673, Accuracy: 77.75%\n",
      "Epoch: 7, Batch: 8000, Loss: 0.669, Accuracy: 77.68%\n",
      "Epoch: 7, Batch: 10000, Loss: 0.672, Accuracy: 77.74%\n",
      "Epoch: 7, Batch: 12000, Loss: 0.686, Accuracy: 77.65%\n",
      "Epoch: 8, Batch: 2000, Loss: 0.609, Accuracy: 80.20%\n",
      "Epoch: 8, Batch: 4000, Loss: 0.627, Accuracy: 79.78%\n",
      "Epoch: 8, Batch: 6000, Loss: 0.630, Accuracy: 79.44%\n",
      "Epoch: 8, Batch: 8000, Loss: 0.625, Accuracy: 79.41%\n",
      "Epoch: 8, Batch: 10000, Loss: 0.627, Accuracy: 79.45%\n",
      "Epoch: 8, Batch: 12000, Loss: 0.630, Accuracy: 79.46%\n",
      "Epoch: 9, Batch: 2000, Loss: 0.557, Accuracy: 81.47%\n",
      "Epoch: 9, Batch: 4000, Loss: 0.566, Accuracy: 81.54%\n",
      "Epoch: 9, Batch: 6000, Loss: 0.570, Accuracy: 81.45%\n",
      "Epoch: 9, Batch: 8000, Loss: 0.577, Accuracy: 81.27%\n",
      "Epoch: 9, Batch: 10000, Loss: 0.576, Accuracy: 81.28%\n",
      "Epoch: 9, Batch: 12000, Loss: 0.584, Accuracy: 81.19%\n",
      "Epoch: 10, Batch: 2000, Loss: 0.515, Accuracy: 83.08%\n",
      "Epoch: 10, Batch: 4000, Loss: 0.506, Accuracy: 83.03%\n",
      "Epoch: 10, Batch: 6000, Loss: 0.528, Accuracy: 82.97%\n",
      "Epoch: 10, Batch: 8000, Loss: 0.529, Accuracy: 82.87%\n",
      "Epoch: 10, Batch: 10000, Loss: 0.536, Accuracy: 82.86%\n",
      "Epoch: 10, Batch: 12000, Loss: 0.527, Accuracy: 82.84%\n",
      "Epoch: 11, Batch: 2000, Loss: 0.461, Accuracy: 84.79%\n",
      "Epoch: 11, Batch: 4000, Loss: 0.490, Accuracy: 84.24%\n",
      "Epoch: 11, Batch: 6000, Loss: 0.476, Accuracy: 84.20%\n",
      "Epoch: 11, Batch: 8000, Loss: 0.503, Accuracy: 84.15%\n",
      "Epoch: 11, Batch: 10000, Loss: 0.513, Accuracy: 83.89%\n",
      "Epoch: 11, Batch: 12000, Loss: 0.498, Accuracy: 83.86%\n",
      "Epoch: 12, Batch: 2000, Loss: 0.420, Accuracy: 86.46%\n",
      "Epoch: 12, Batch: 4000, Loss: 0.452, Accuracy: 85.92%\n",
      "Epoch: 12, Batch: 6000, Loss: 0.453, Accuracy: 85.60%\n",
      "Epoch: 12, Batch: 8000, Loss: 0.438, Accuracy: 85.57%\n",
      "Epoch: 12, Batch: 10000, Loss: 0.463, Accuracy: 85.34%\n",
      "Epoch: 12, Batch: 12000, Loss: 0.459, Accuracy: 85.29%\n",
      "Epoch: 13, Batch: 2000, Loss: 0.398, Accuracy: 87.04%\n",
      "Epoch: 13, Batch: 4000, Loss: 0.430, Accuracy: 86.49%\n",
      "Epoch: 13, Batch: 6000, Loss: 0.397, Accuracy: 86.62%\n",
      "Epoch: 13, Batch: 8000, Loss: 0.420, Accuracy: 86.51%\n",
      "Epoch: 13, Batch: 10000, Loss: 0.415, Accuracy: 86.49%\n",
      "Epoch: 13, Batch: 12000, Loss: 0.410, Accuracy: 86.48%\n",
      "Epoch: 14, Batch: 2000, Loss: 0.359, Accuracy: 88.25%\n",
      "Epoch: 14, Batch: 4000, Loss: 0.352, Accuracy: 88.41%\n",
      "Epoch: 14, Batch: 6000, Loss: 0.378, Accuracy: 88.20%\n",
      "Epoch: 14, Batch: 8000, Loss: 0.364, Accuracy: 88.14%\n",
      "Epoch: 14, Batch: 10000, Loss: 0.376, Accuracy: 88.08%\n",
      "Epoch: 14, Batch: 12000, Loss: 0.394, Accuracy: 87.95%\n",
      "Epoch: 15, Batch: 2000, Loss: 0.339, Accuracy: 88.74%\n",
      "Epoch: 15, Batch: 4000, Loss: 0.334, Accuracy: 88.98%\n",
      "Epoch: 15, Batch: 6000, Loss: 0.342, Accuracy: 88.95%\n",
      "Epoch: 15, Batch: 8000, Loss: 0.366, Accuracy: 88.69%\n",
      "Epoch: 15, Batch: 10000, Loss: 0.345, Accuracy: 88.64%\n",
      "Epoch: 15, Batch: 12000, Loss: 0.362, Accuracy: 88.62%\n",
      "Epoch: 16, Batch: 2000, Loss: 0.293, Accuracy: 90.54%\n",
      "Epoch: 16, Batch: 4000, Loss: 0.308, Accuracy: 90.06%\n",
      "Epoch: 16, Batch: 6000, Loss: 0.317, Accuracy: 89.85%\n",
      "Epoch: 16, Batch: 8000, Loss: 0.335, Accuracy: 89.67%\n",
      "Epoch: 16, Batch: 10000, Loss: 0.317, Accuracy: 89.61%\n",
      "Epoch: 16, Batch: 12000, Loss: 0.326, Accuracy: 89.54%\n",
      "Epoch: 17, Batch: 2000, Loss: 0.274, Accuracy: 91.04%\n",
      "Epoch: 17, Batch: 4000, Loss: 0.286, Accuracy: 90.95%\n",
      "Epoch: 17, Batch: 6000, Loss: 0.300, Accuracy: 90.72%\n",
      "Epoch: 17, Batch: 8000, Loss: 0.303, Accuracy: 90.66%\n",
      "Epoch: 17, Batch: 10000, Loss: 0.297, Accuracy: 90.56%\n",
      "Epoch: 17, Batch: 12000, Loss: 0.301, Accuracy: 90.47%\n",
      "Epoch: 18, Batch: 2000, Loss: 0.235, Accuracy: 92.58%\n",
      "Epoch: 18, Batch: 4000, Loss: 0.275, Accuracy: 91.84%\n",
      "Epoch: 18, Batch: 6000, Loss: 0.261, Accuracy: 91.69%\n",
      "Epoch: 18, Batch: 8000, Loss: 0.273, Accuracy: 91.51%\n",
      "Epoch: 18, Batch: 10000, Loss: 0.262, Accuracy: 91.50%\n",
      "Epoch: 18, Batch: 12000, Loss: 0.284, Accuracy: 91.32%\n",
      "Epoch: 19, Batch: 2000, Loss: 0.234, Accuracy: 92.41%\n",
      "Epoch: 19, Batch: 4000, Loss: 0.235, Accuracy: 92.36%\n",
      "Epoch: 19, Batch: 6000, Loss: 0.246, Accuracy: 92.22%\n",
      "Epoch: 19, Batch: 8000, Loss: 0.253, Accuracy: 92.06%\n",
      "Epoch: 19, Batch: 10000, Loss: 0.245, Accuracy: 92.13%\n",
      "Epoch: 19, Batch: 12000, Loss: 0.262, Accuracy: 92.01%\n",
      "Epoch: 20, Batch: 2000, Loss: 0.205, Accuracy: 93.27%\n",
      "Epoch: 20, Batch: 4000, Loss: 0.228, Accuracy: 92.89%\n",
      "Epoch: 20, Batch: 6000, Loss: 0.229, Accuracy: 92.81%\n",
      "Epoch: 20, Batch: 8000, Loss: 0.219, Accuracy: 92.84%\n",
      "Epoch: 20, Batch: 10000, Loss: 0.229, Accuracy: 92.81%\n",
      "Epoch: 20, Batch: 12000, Loss: 0.222, Accuracy: 92.83%\n",
      "Epoch: 21, Batch: 2000, Loss: 0.181, Accuracy: 94.20%\n",
      "Epoch: 21, Batch: 4000, Loss: 0.208, Accuracy: 93.93%\n",
      "Epoch: 21, Batch: 6000, Loss: 0.205, Accuracy: 93.83%\n",
      "Epoch: 21, Batch: 8000, Loss: 0.214, Accuracy: 93.67%\n",
      "Epoch: 21, Batch: 10000, Loss: 0.206, Accuracy: 93.58%\n",
      "Epoch: 21, Batch: 12000, Loss: 0.239, Accuracy: 93.38%\n",
      "Epoch: 22, Batch: 2000, Loss: 0.176, Accuracy: 94.21%\n",
      "Epoch: 22, Batch: 4000, Loss: 0.201, Accuracy: 94.11%\n",
      "Epoch: 22, Batch: 6000, Loss: 0.172, Accuracy: 94.30%\n",
      "Epoch: 22, Batch: 8000, Loss: 0.208, Accuracy: 94.03%\n",
      "Epoch: 22, Batch: 10000, Loss: 0.198, Accuracy: 93.97%\n",
      "Epoch: 22, Batch: 12000, Loss: 0.199, Accuracy: 93.89%\n",
      "Epoch: 23, Batch: 2000, Loss: 0.155, Accuracy: 94.83%\n",
      "Epoch: 23, Batch: 4000, Loss: 0.179, Accuracy: 94.51%\n",
      "Epoch: 23, Batch: 6000, Loss: 0.180, Accuracy: 94.38%\n",
      "Epoch: 23, Batch: 8000, Loss: 0.175, Accuracy: 94.35%\n",
      "Epoch: 23, Batch: 10000, Loss: 0.195, Accuracy: 94.28%\n",
      "Epoch: 23, Batch: 12000, Loss: 0.185, Accuracy: 94.19%\n",
      "Epoch: 24, Batch: 2000, Loss: 0.141, Accuracy: 95.43%\n",
      "Epoch: 24, Batch: 4000, Loss: 0.172, Accuracy: 95.04%\n",
      "Epoch: 24, Batch: 6000, Loss: 0.175, Accuracy: 94.85%\n",
      "Epoch: 24, Batch: 8000, Loss: 0.162, Accuracy: 94.79%\n",
      "Epoch: 24, Batch: 10000, Loss: 0.175, Accuracy: 94.73%\n",
      "Epoch: 24, Batch: 12000, Loss: 0.174, Accuracy: 94.73%\n",
      "Epoch: 25, Batch: 2000, Loss: 0.135, Accuracy: 95.55%\n",
      "Epoch: 25, Batch: 4000, Loss: 0.148, Accuracy: 95.34%\n",
      "Epoch: 25, Batch: 6000, Loss: 0.135, Accuracy: 95.43%\n",
      "Epoch: 25, Batch: 8000, Loss: 0.163, Accuracy: 95.30%\n",
      "Epoch: 25, Batch: 10000, Loss: 0.153, Accuracy: 95.30%\n",
      "Epoch: 25, Batch: 12000, Loss: 0.155, Accuracy: 95.20%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # get the predicted class labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()  # compare with the true labels\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {(running_loss / 2000):.3f}, Accuracy: {(correct / total) * 100:.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c74a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(net.state_dict(), './cifar10_95.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8364f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
